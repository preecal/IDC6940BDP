---
title: "LINEAR MIXED MODELS"
subtitle: "Introduction to linear mixed models"
advisor: "Dr.Seals"
author: "Preethi Ravikumar"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science - IDC6940
bibliography: references.bib # file contains bibtex for references
always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
  
---
    
Slides: [Preethi_Slides.html](Preethi_Slides.html){target="_blank"} ( Go to `Preethi_Slides.qmd`
to edit)

  Statistical modeling is a cornerstone of quantitative research, enabling researchers to understand relationships and draw inferences from data. However, traditional methods such as analysis of variance (ANOVA) are often limited when applied to datasets with hierarchical structures or repeated measurements. These methods assume independence among observations and fail to account for variations within and between groups, leading to biased results or loss of valuable information.

  To address these shortcomings, linear mixed models were developed as a more versatile and robust alternative. By incorporating random effects alongside fixed effects, LMMs allow researchers to model variability at multiple levels, account for correlations within groups, and handle unbalanced data with ease. 


## Limitations of ANOVA, Simple Linear Models and the Role of LMM:

Linear mixed models (LMMs) are used when data have some form of grouping or hierarchical structure, where observations within groups are not independent. 

Traditional linear models (like simple linear regression or ANOVA) fail to account for this structure, which can lead to inaccurate inferences and incorrect estimates of variability.This assume **independence** and may exclude data through **listwise deletion** (Barr, 2008; Enders, 2010)

Here’s why linear mixed models are necessary and what problems arise with simple linear models and ANOVA.

| Feature                     | Simple Linear Models / ANOVA              | Linear Mixed Models                                   |
|-------------------------    |----------------------------------------   |--------------------------------------------------     |
| Assumption of independence  | Assumes all observations are independent  | Accounts for correlation within groups                |
| Variability across groups   | Assumes homogeneous variance              | Can model heterogeneous variance                      |
| Fixed vs. Random effects    | Treats all effects as fixed               | Distinguishes between fixed and random effects        |
| Small sample groups         | No shrinkage; separate estimates          | Shrinkage/partial pooling improves estimates for small groups |
| Repeated measures           | Cannot handle repeated measures well      | Models within-subject correlations                    |


## Components of Linear Mixed Models:  {.smaller}

A *linear mixed model* (LMM) in statistics is a type of statistical model used to account for both fixed and random effects in data that might have some form of hierarchical or clustered structure. It extends the standard linear regression model by incorporating random effects, which allows for more flexibility in modeling data where observations may not be independent of each other.

 *Components of a Normal Linear Mixed Model:*

1. *Fixed Effects*: These represent the population-wide average effects of the predictors. The parameters associated with fixed effects are considered constant across all individuals or groups.
   
   - For example, in a study of student test scores, a fixed effect might be the influence of teaching method on scores, assuming the method affects all students similarly. (Gelman & Hill, 2007)

2. *Random Effects*: These capture the variability specific to individual units (such as subjects, groups, or clusters) and assume that these effects are randomly sampled from a population distribution.
   
   - In the same study of test scores, random effects could represent differences in baseline scores between different schools or individual students. (Baayen et al., 2008)

3. *Residual (Error) Term*: This represents the unexplained variability in the data after accounting for both fixed and random effects. In the case of a **normal linear mixed model*, this error term is assumed to follow a normal distribution.


## Model Equation:  {.smaller}

The general form of the normal linear mixed model can be written as:


y = X $\beta$ + Z $\mu$ + $\epsilon$ 


Where:
- \( y \) is the response variable (the dependent variable,
- \( X $\beta$) represents the fixed effects (with \( X \) being the design matrix for fixed effects and $\beta$  the coefficients,

- \( Z $\mu$ \) represents the random effects (with \( Z \) being the design matrix for random effects and $\mu$ the random effect coefficients,

-  $\epsilon$   is the residual error term, typically assumed to follow a normal distribution.

*Assumptions:*

  - The residuals $\epsilon$ are normally distributed with mean zero and constant variance $\sigma$\^2\ .
  - The random effects $\mu$ are also assumed to follow a normal distribution, often with a mean of zero and some variance-covariance structure.

*Applications:*

LMMs are widely used in situations where there is clustering or repeated measures, such as:
  - *Longitudinal data analysis* (e.g., repeated measurements over time for the same individuals),
  - *Hierarchical data* (e.g., students within schools, patients within hospitals),
  - *Multilevel models* for group-level data. (Bruin, 2006; Bates, 2014)

By incorporating both fixed and random effects, NLMMs can provide more accurate and nuanced estimates in settings where data may not be independent or homoscedastic (i.e., having constant variance).

## Covariance Structure {.smaller} 

We talk about the covariance structure in linear mixed models because it is essential for accurately modeling relationships, correlations, and variability in the data. A correctly specified covariance structure ensures that LMMs provide valid inferences and better predictions, especially for complex datasets with hierarchical or repeated measurements. Choosing the right structure is essential for modeling repeated measures (Starkweather, 2010)

Covariance measures joint variability — the extent of variation between two random variables. 

It is similar to variance, but while variance quantifies the variability of a single variable, covariance quantifies how two variables vary together.

The measure can be positive, negative, or zero:

•	Positive covariance = an overall tendency for variables to move together. Data points will trend upwards on a graph.

•	Negative covariance = a overall tendency that when one variable increases, so does the other. Data points will trend downward on a graph.

A high covariance indicates a strong relationship between the variables, while a low value suggests a weak relationship. However, unlike the correlation coefficient — which ranges from 0 to 1 
Covariance has no limitations on its values, which can make it challenging to interpret. 


- What is Covariance structure ?

    In a mixed model, a covariance structure describes the pattern of correlations between repeated measurements taken on the same subject, essentially defining how the errors within a group (like individuals in a study) are related to each other, allowing for more accurate analysis of data with clustered or hierarchical structures, especially when dealing with longitudinal or repeated measures data

- Why is it necessary for modeling repeated measures?

    Modeling repeated measures in mixed models is necessary because when you have multiple observations taken from the same subject over time, these observations are likely to be correlated with each other, and a standard linear regression model that assumes independence of observations would not accurately account for this correlation, potentially leading to unreliable results; 

Mixed models allows to explicitly model this within-subject dependence by incorporating random effects specific to each individual, providing a more accurate analysis of the data


## Mixed Models for Repeated Measures {.smaller}

  Cluster randomized trials (CRTs) are a design used to test interventions where individual randomization is not appropriate. The mixed model for repeated measures (MMRM) is a popular choice for individually randomized trials with longitudinal continuous outcomes. This model’s appeal is due to avoidance of model misspecification and its unbiasedness for data missing completely at random or at random.

  Trials with longitudinally measured outcomes have two sources of non-independence: the cluster and the repeated measures over time. 

  Linear mixed-effects models are one option for handling the non-independence of measurements over time.In this context, one may use a random-coefficients model, using random effects for a subject’s intercept and sometimes slope. Alternatively, one may use covariance pattern models, where the covariance between repeated measures on the same subject is modeled explicitly from the residual effects.

The mixed model for repeated measures uses an unstructured time and covariance structure. 

-	Unstructured time means that time is modeled categorically, rather than continuously as a linear or polynomial function, and allows for an arbitrary trajectory over time. 

-	While the continuous time models may use fewer degrees of freedom and may, therefore, be more powerful, it can be difficult to anticipate the outcome’s time trajectory in advance. 

## Robust Estimation {.smaller}

What is Robust Estimation? 

  Robust estimation refers to statistical methods designed to provide reliable parameter estimates even when the data contains outliers or deviations from standard assumptions (such as normality). These methods aim to minimize the influence of such outliers, which can significantly distort the results of traditional statistical techniques. They are less sensitive to extreme values or outliers, providing estimates that are more representative of the majority of the data.Typically have a high breakdown point, meaning they can handle a significant percentage of outliers without breaking down. Composite robust estimators improve model accuracy when handling *outliers* (Agostinelli, 2016).

What is Non-Robust Estimation?

  Non-robust estimation refers to traditional statistical methods that assume the data are free from outliers or adhere strictly to specified distributions. These methods can produce unreliable estimates when the data contain outliers.These methods, such as ordinary least squares (OLS) regression, can be heavily influenced by a small number of extreme values, leading to biased or misleading results. They typically have a lower breakdown point, meaning that even a small proportion of outliers can lead to failure in obtaining valid estimates and often rely on strict assumptions regarding the underlying data distribution, such as normality or homoscedasticity.

**Robust methods**

  Robust methods are preferred in situations where data may contain outliers or when the underlying assumptions are violated. Non-robust methods are typically more efficient under ideal conditions (no outliers and met assumptions).In the presence of outliers, robust estimators maintain consistent performance, while non-robust estimators may yield skewed or incorrect results.

## Dataset for used for analysis

Dataset: OASIS-Longitudinal MRI Data in Nondemented and Demented Older Adults

Source: NCBI

Summary of the chosen dataset:

  - Includes MRI scans of 150 subjects aged 60-96.
  
  - Longitudinal data collected over 373 sessions.
  
  - Subjects scanned for MRI at least twice, with visits separated by at least a year.
  
  - Each session had 3-4 T1-weighted MRI scans.
  
  - Clinical Dementia Rating (CDR) categorized as nondemented or with mild Alzheimer’s disease.
  
  - 72 were nondemented throughout, while 64 were demented initially and remained so.
  
  - Participants were right-handed, consisting of 62 men and 88 women.
  
  Epidemiologic and clinical research papers often describe the study sample in the first table. Table 1 can illuminate potential threats to internal and external validity. “Who is in this study?” is the first question many readers of clinical and epidemiologic studies ask. Readers care about who is in a study because it helps them understand and evaluate the study’s findings: to assess applicability to other patients or populations (i.e. external validity), and risk of bias (i.e. internal validity).1,2 As a result, papers often include a table that describes the study sample; this is commonly the first table in a paper. This “Table 1,” as it is colloquially called, can be designed to shed light on potential threats to both internal and external validity of study findings.


# R script for table1 {.smaller}

```{r, warning=FALSE, echo=T, message=FALSE, result ='hide'}

#install.packages("tableone")
library(tableone)

#install.packages("readr")
library(readr)

oasis_longitudinal <- read_csv("oasis_longitudinal.csv")

# Load necessary library
library(tableone)

# Recode M/F for easier handling in Table 1 (optional)
oasis_longitudinal$Gender <- ifelse(oasis_longitudinal$'M/F' == "M", "Male", "Female")

# Define the variables to be included in Table 1
vars <- c("Age", "Gender", "Hand", "EDUC", "SES", "MMSE", "CDR", "eTIV", "nWBV", "ASF")

# Define the stratifying variable (Group)
strata <- "Group"

# Create Table 1
table1 <- CreateTableOne(vars = vars, strata = strata, data = oasis_longitudinal, factorVars = c("Gender", "Hand", "CDR"))

# Print the table with p-values
print(table1, showAllLevels = TRUE, smd = TRUE)

```

# Table1 {.smaller}

Role of Table 1:

Table 1 provides a snapshot of the key characteristics of the study sample (e.g., demographic, clinical, and cognitive variables). This helps establish a clear understanding of the dataset’s composition.
If your analysis involves comparing groups (e.g., by dementia status: demented vs. nondemented), Table 1 allows you to display differences across groups at baseline.
By summarizing the data, Table 1 ensures that the sample is representative and aligns with the research objectives.
It helps identify any major imbalances or biases between groups, which can influence statistical analysis (e.g., differences in age or socioeconomic status between groups).


```{=html}
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-w7ak{border-color:#ffffff;font-family:Georgia, serif !important;text-align:left;vertical-align:top}
</style>
```
|           | level | Converted        | Demented         | Nondemented      | p       |
|------------|------------|------------|------------|------------|------------|
| n         |       | 37               | 146              | 190              |         |
| Female    | F     |      24 ( 64.9)  |      60 ( 41.1)  |     129 ( 67.9)  | \<0.001 |
| Male      | M     |      13 ( 35.1)  |      86 ( 58.9)  |      61 ( 32.1)  |         |
| Dom Hand  | R     |      37 (100.0)  |     146 (100.0)  |     190 (100.0)  |     NA  |
| CDR       | 0     |      18 ( 48.6)  |       0 (  0.0)  |     188 ( 98.9)  | \<0.001 |
|           | 0.5   |      19 ( 51.4)  |     102 ( 69.9)  |       2 (  1.1)  |         |
|           | 1     |       0 (  0.0)  |      41 ( 28.1)  |       0 (  0.0)  |         |
|           | 2     |       0 (  0.0)  |       3 (  2.1)  |       0 (  0.0)  |         |
| Age       |       |   79.76 (7.43)   |   76.26 (6.94)   |   77.06 (8.10)   | 0.045   |
| Education |       |   15.46 (2.52)   |   13.67 (2.90)   |   15.14 (2.74)   | \<0.001 |
| SES       |       |    1.73 (0.96)   |    2.77 (1.20)   |    2.39 (1.05)   | \<0.001 |
| MMSE      |       |   28.68 (1.56)   |   24.51 (4.50)   |   29.23 (0.88)   | \<0.001 |
| eTIV      |       | 1459.27 (135.43) | 1485.85 (173.77) | 1495.50 (184.89) | 0.51    |
| nWBV      |       |    0.72 (0.04)   |    0.72 (0.03)   |    0.74 (0.04)   | \<0.001 |
| ASF       |       |    1.21 (0.11)   |    1.20 (0.14)   |    1.19 (0.14)   | 0.683   |

## Data Visualizations 

# Age of Participants
```{r, warning=FALSE, echo=F, message=FALSE}

counts <- table(oasis_longitudinal$Age)
barplot(counts, main="Age of Participants",
   xlab="Age")
```
Participants’ ages range from approximately 60 to 96 years.
The most frequent age group is between 75 and 80, suggesting that a significant portion of participants falls within this range.
The distribution appears slightly right-skewed, with fewer participants in the older age ranges (above 85).
There is a wide variability in age, indicating that the dataset includes individuals from a broad spectrum of older adults, which is beneficial for studying aging and dementia trends.

# Gender of Participants
```{r, warning=FALSE, echo=F, message=FALSE}
counts <- table(oasis_longitudinal$`M/F`)
barplot(counts, main="Gender of Participants",
   xlab="Gender")
```

From this graph we can infer that in this study the majority of the participants are female.

## Normalize Whole Brain Volume vs Age
```{r, warning=FALSE, echo=F, message=FALSE}
#| echo: false
x <- oasis_longitudinal$nWBV
y <- oasis_longitudinal$Age
plot(x, y, main = "Normalize Whole Brain Volume vs Age",
     xlab = "nWBV", ylab = "Age",
     pch = 19, frame = FALSE)
abline(lm(y ~ x, data = oasis_longitudinal), col = "blue")
    
```
The plot shows a clear negative relationship between nWBV and age. As age increases, normalized whole brain volume tends to decrease. This trend is highlighted by the fitted regression line (blue).
The gradual decline in nWBV with age is consistent with the known biological phenomenon of brain atrophy in aging populations.
There is considerable variability in nWBV at each age, suggesting that other factors (e.g., cognitive health, lifestyle, genetics) might influence brain volume.
This trend supports the importance of considering brain volume as a variable in studies on cognitive decline and dementia. Individuals with lower nWBV at the same age might exhibit greater risk or severity of cognitive impairment.

## Analysis - Impact of Age and MR Delay on Brain Volume (nWBV)

Fitting the Linear Mixed Model:

-   Outcome: nWBV (Normalized Whole Brain Volume) – structural brain
    changes over time.

-   Fixed Effects:
    -   Primary Predictor: Age correlates with changes in brain volume.
    -   Secondary Predictor: MR Delay impact brain volume over time.

-   Random Effects:
    -   Subject.ID.

The linear mixed model used in this analysis is given by:

$$
\text{nWBV}{ij} = \beta_0 + \beta_1 \cdot \text{Age}{ij} + \beta_2 \cdot \text{MRDelay}{ij} + \mu_{i} + \epsilon_{ij}
$$

Where:

-   $nWBV_{ij}$ : Normalized whole brain volume for subject (i) at time
    (j).
-   $\beta_0$ : Overall intercept (fixed effect).
-   $\beta_1$ : Fixed effect of Age.
-   $\beta_2$ : Fixed effect of MR Delay.
-   $\mu_{i}$ : Random intercept for each subject.
-   $\epsilon_{ij}$ : Residual error term.

## METHODOLOGY {.smaller}

The most common method used in fitting linear mixed models are:

**1. Maximum Likelihood Estimation (MLE)**:

  It determines the parameters under which the observed data is most probable. This method estimates both the fixed effects (population-level parameters) and variance components (random effects and residual variances) by maximizing the likelihood of the observed data.

**2. Restricted Maximum Likelihood Estimation (REML)** :

  It maximizes the likelihood of the data after adjusting for the fixed effects, focusing on variance    components estimation. This method is less biased because it adjusts for the loss of degrees of freedom caused by estimating fixed effects.

## 1. Age as the only predictor {.smaller}
-   lmer function uses REML unless specified otherwise
-   data cleaning
```{r, warning=FALSE, echo=FALSE, message=FALSE}
#install.packages("readr")
library(readr)

oasis_data <- read_csv("oasis_longitudinal.csv")

# Remove duplicate rows
oasis_data <- oasis_data[!duplicated(oasis_data), ]

# Remove rows with missing values
oasis_data <- na.omit(oasis_data)

# View cleaned data
head(oasis_data)
```

```{r, warning=FALSE, echo=T, message=FALSE}
#install.packages("lme4")
library(lme4)
oasis_data$gender <- oasis_data$'M/F'
oasis_data$SubjectID <- oasis_data$'Subject ID'
oasis_data$MRDelay <- oasis_data$`MR Delay`

# Fit the linear mixed model with only Age as the predictor, reml
model1 <- lmer(nWBV ~ Age + (1 | SubjectID), data = oasis_data)
summary(model1)
```

# Equation and Interpretation of Age as the only predictor {.smaller}

the linear mixed equation is:
$$
\text{nWBV}{ij} = 0.9978 - 0.0035 \cdot \text{Age}{ij} + \mu_{i} + \epsilon_{ij}
$$

where:

• 0.9978 is the fixed intercept, representing the average baseline nWBV
when Age is 0.

• -0.0035 is the fixed effect estimate for Age, indicating that for each
additional year of age, the nWBV decreases by approximately 0.0035 units
on average.

**INTERPRETATION**:

-   age has a significant negative effect on nWBV
-   t-value for Age = -15.78
-   this shows a strong association between age and decreasing nWBV.

## 2.Age and MR Delay as predictors

```{r, warning=FALSE, echo=T, message=FALSE}
# Fit Linear Mixed Model with Age and MR Delay as the predictors
model2 <- lmer(nWBV ~ Age + MRDelay + (1 | SubjectID), data = oasis_data)
summary(model2)
```
**Equation and Interpretation of Age and MR Delay:**

The model equation: 
$$
\text{nWBV}{ij} = 0.9296 - 0.0026 \cdot \text{Age}{ij} - 0.0000043 \cdot \text{MRDelay}{ij} + \mu_{i} + \epsilon_{ij}
$$ where:

• 0.9296 is the intercept, representing the estimated nWBV when both Age
and MR Delay are 0.

• -0.0026 is the coefficient for Age, indicating that each additional
year of age is associated with an average decrease in nWBV by
approximately 0.0026 units.

• -0.0000043 is the coefficient for MR Delay, suggesting that for every
one-day increase in MR Delay, nWBV decreases by .0000043

**INTERPRETATION**


-   Age
    -   strong, statistically significant negative effect on nWBV
    -   with a absolute t value of 7.57

-   MR Delay
    -   small neagtive effect on nWBV with an absolute t-value = 3.521

## Effect of Age and MR Delay on nWBV {.smaller}

```{r, warning=FALSE, echo=F, message=FALSE, }
#install.packages("effects")
library(effects)

# Generate effect plots
effect_plot <- allEffects(model2) 
plot(effect_plot)
```
Age Effect Plot:

  - Negative Relationship: nWBV decreases with Age
  - Confidence Intervals : narrower interval in the middle suggests the estimate is more reliable for the majority of observations
  
MR Delay Effect Plot:
  
  - Negative Relationship: a longer delay between MRI scans may correspond to a lower nWBV 
  - Confidence Intervals : The shaded area is wider than in the Age plot, indicating more uncertainty in estimating the effect of MR Delay compared to Age.
  
## Maximum Likelihood (predictors: Age and MR Delay) {.smaller}
```{r, warning=FALSE, echo=T, message=FALSE}
model_ml <- lmer(nWBV ~ Age + MRDelay + (1 | SubjectID), data = oasis_data, REML = FALSE)
summary(model_ml)
```
**Results of MLE:**

The coefficients for Age and MRDelay are almost identical between MLE and REML in your case, indicating that the fixed effects are estimated consistently by both methods. 

## Comparison of the two methods:

 *Fixed Effects*: The estimates for Age and MR Delay under REML are nearly identical to those under ML, showing consistent results.

 **Age** has a significant negative impact on nWBV, indicating
**cognitive decline with age**.

 The inclusion of **MRDelay** in the model is still relevant because even small effects can contribute to understanding the variation in nWBV.

In summary,

   REML is appropriate for final model interpretation as it provides the better estimates for variance components,while both methods confirm **Age as a key predictor of nWBV decline**.


## References {.smaller}

- Barr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. *Journal of Memory and Language*, 68(3), 255-278.

- Enders, C. K. (2010). *Applied Missing Data Analysis*. Guilford Press.

- Agostinelli, C., & Yohai, V. J. (2016). Composite robust estimators for linear mixed models. *Journal of the American Statistical Association*, 111(516), 1764-1774. 

- Gelman, A., & Hill, J. (2007). *Data Analysis Using Regression and Multilevel/Hierarchical Models*.    Cambridge University Press.

- Baayen, R. H., Davidson, D. J., & Bates, D. M. (2008). Mixed-effects modeling with crossed random effects   for subjects and items. *Journal of Memory and Language*, 59(4), 390-412.

- Bruin, J. (2006). newtest: command to compute new test. UCLA: Statistical Consulting Group. 

- Bates, D. (2014). Fitting linear mixed-effects models using lme4. arXiv preprint arXiv:1406.5823.

- Starkweather, J. (2010). Linear mixed effects modeling using R. Unpublished Manuscript.

- Fitzmaurice GM, Laird NM, Ware JH. Applied longitudinal analysis. 2nd ed. Hoboken: Wiley; 2011.

- dataset: https://pmc.ncbi.nlm.nih.gov/articles/PMC2895005/

- An introduction to Linear Mixed-Effects Modelling in R (Author: Violet A. Brown)
  https://journals.sagepub.com/doi/10.1177/2515245920960351

- Introduction to Linear Models (from UCLA)
  https://stats.oarc.ucla.edu/other/mult-pkg/introduction-to-linear-mixed-models/#:~:text=Linear%20mixed%20models%20are%20an,arises%20from%20a%20hierarchical%20structure
